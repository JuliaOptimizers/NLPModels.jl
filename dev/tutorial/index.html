<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · NLPModels.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="NLPModels.jl logo"/></a><h1>NLPModels.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../models/">Models</a></li><li><a class="toctext" href="../tools/">Tools</a></li><li class="current"><a class="toctext" href>Tutorial</a><ul class="internal"><li><a class="toctext" href="#ADNLPModel-Tutorial-1">ADNLPModel Tutorial</a></li><li><a class="toctext" href="#Nonlinear-least-squares-models-1">Nonlinear least squares models</a></li></ul></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../reference/">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Tutorial</a></li></ul><a class="edit-page" href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl/blob/master/docs/src/tutorial.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Tutorial</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Tutorial-1" href="#Tutorial-1">Tutorial</a></h1><ul><li><a href="#Tutorial-1">Tutorial</a></li><ul><li><a href="#ADNLPModel-Tutorial-1">ADNLPModel Tutorial</a></li><li><a href="#Nonlinear-least-squares-models-1">Nonlinear least squares models</a></li></ul></ul><p>NLPModels.jl was created for two purposes:</p><ul><li>Allow users to access problem databases in an unified way.</li></ul><p>Mainly, this means  <a href="https://github.com/JuliaSmoothOptimizers/CUTEst.jl">CUTEst.jl</a>,  but it also gives access to <a href="https://github.com/JuliaSmoothOptimizers/AmplNLReader.jl">AMPL  problems</a>,  as well as JuMP defined problems (e.g. as in  <a href="https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl">OptimizationProblems.jl</a>).</p><ul><li>Allow users to create their own problems in the same way.</li></ul><p>As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,  <a href="https://github.com/JuliaSmoothOptimizers/Optimize.jl">Optimize.jl</a>.</p><p>The main interface for user defined problems is <a href="../models/#ADNLPModel-1">ADNLPModel</a>, which defines a model easily, using automatic differentiation.</p><h2><a class="nav-anchor" id="ADNLPModel-Tutorial-1" href="#ADNLPModel-Tutorial-1">ADNLPModel Tutorial</a></h2><p>ADNLPModel is simple to use and is useful for classrooms. It only needs the objective function <span>$f$</span> and a starting point <span>$x^0$</span> to be well-defined. For constrained problems, you&#39;ll also need the constraints function <span>$c$</span>, and the constraints vectors <span>$c_L$</span> and <span>$c_U$</span>, such that <span>$c_L \leq c(x) \leq c_U$</span>. Equality constraints will be automatically identified as those indices <span>$i$</span> for which <span>$c_{L_i} = c_{U_i}$</span>.</p><p>Let&#39;s define the famous Rosenbrock function</p><div>\[f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2,\]</div><p>with starting point <span>$x^0 = (-1.2,1.0)$</span>.</p><div><pre><code class="language-julia">using NLPModels

nlp = ADNLPModel(x-&gt;(x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2 , [-1.2; 1.0])</code></pre><pre><code class="language-none">ADNLPModel(Minimization problem Generic
nvar = 2, ncon = 0 (0 linear)
, Counters(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), getfield(Main.ex-adnlp, Symbol(&quot;##1#2&quot;))(), getfield(NLPModels, Symbol(&quot;##111#114&quot;))())</code></pre></div><p>This is enough to define the model. Let&#39;s get the objective function value at <span>$x^0$</span>, using only <code>nlp</code>.</p><div><pre><code class="language-julia">fx = obj(nlp, nlp.meta.x0)
println(&quot;fx = $fx&quot;)</code></pre><pre><code class="language-none">fx = 24.199999999999996</code></pre></div><p>Done. Let&#39;s try the gradient and Hessian.</p><div><pre><code class="language-julia">gx = grad(nlp, nlp.meta.x0)
Hx = hess(nlp, nlp.meta.x0)
println(&quot;gx = $gx&quot;)
println(&quot;Hx = $Hx&quot;)</code></pre><pre><code class="language-none">gx = [-215.6, -88.0]
Hx = [1330.0 0.0; 480.0 200.0]</code></pre></div><p>Notice how only the lower triangle of the Hessian is stored. Also notice that it is <em>dense</em>. This is a current limitation of this model. It doesn&#39;t return sparse matrices, so use it with care.</p><p>Let&#39;s do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method</p><ol><li>Given <span>$x^0$</span>, <span>$\varepsilon &gt; 0$</span>, and <span>$\eta \in (0,1)$</span>. Set <span>$k = 0$</span>;</li><li>If <span>$\Vert \nabla f(x^k) \Vert &lt; \varepsilon$</span> STOP with <span>$x^* = x^k$</span>;</li><li>Compute <span>$d^k = -\nabla f(x^k)$</span>;</li><li>Compute <span>$\alpha_k \in (0,1]$</span> such that <span>$f(x^k + \alpha_kd^k) &lt; f(x^k) + \alpha_k\eta \nabla f(x^k)^Td^k$</span></li><li>Define <span>$x^{k+1} = x^k + \alpha_kx^k$</span></li><li>Update <span>$k = k + 1$</span> and go to step 2.</li></ol><div><pre><code class="language-julia">using LinearAlgebra

function steepest(nlp; itmax=100000, eta=1e-4, eps=1e-6, sigma=0.66)
  x = nlp.meta.x0
  fx = obj(nlp, x)
  ∇fx = grad(nlp, x)
  slope = dot(∇fx, ∇fx)
  ∇f_norm = sqrt(slope)
  iter = 0
  while ∇f_norm &gt; eps &amp;&amp; iter &lt; itmax
    t = 1.0
    x_trial = x - t * ∇fx
    f_trial = obj(nlp, x_trial)
    while f_trial &gt; fx - eta * t * slope
      t *= sigma
      x_trial = x - t * ∇fx
      f_trial = obj(nlp, x_trial)
    end
    x = x_trial
    fx = f_trial
    ∇fx = grad(nlp, x)
    slope = dot(∇fx, ∇fx)
    ∇f_norm = sqrt(slope)
    iter += 1
  end
  optimal = ∇f_norm &lt;= eps
  return x, fx, ∇f_norm, optimal, iter
end

x, fx, ngx, optimal, iter = steepest(nlp)
println(&quot;x = $x&quot;)
println(&quot;fx = $fx&quot;)
println(&quot;ngx = $ngx&quot;)
println(&quot;optimal = $optimal&quot;)
println(&quot;iter = $iter&quot;)</code></pre><pre><code class="language-none">x = [1.0, 1.0]
fx = 4.2438440239813445e-13
ngx = 9.984661274466946e-7
optimal = true
iter = 17962</code></pre></div><p>Maybe this code is too complicated? If you&#39;re in a class you just want to show a Newton step.</p><div><pre><code class="language-julia">g(x) = grad(nlp, x)
H(x) = Symmetric(hess(nlp, x), :L)
x = nlp.meta.x0
d = -H(x)\g(x)</code></pre><pre><code class="language-none">2-element Array{Float64,1}:
 0.02471910112359557
 0.3806741573033705</code></pre></div><p>or a few</p><div><pre><code class="language-julia">for i = 1:5
  global x
  x = x - H(x)\g(x)
  println(&quot;x = $x&quot;)
end</code></pre><pre><code class="language-none">x = [-1.17528, 1.38067]
x = [0.763115, -3.17503]
x = [0.76343, 0.582825]
x = [0.999995, 0.944027]
x = [0.999996, 0.999991]</code></pre></div><p>Also, notice how we can reuse the method.</p><div><pre><code class="language-julia">f(x) = (x[1]^2 + x[2]^2 - 5)^2 + (x[1]*x[2] - 2)^2
x0 = [3.0; 2.0]
nlp = ADNLPModel(f, x0)

x, fx, ngx, optimal, iter = steepest(nlp)</code></pre><pre><code class="language-none">([2.0, 1.0], 3.911490500207018e-14, 8.979870927068038e-7, true, 153)</code></pre></div><p>Even using a different model. In this case, a model from <a href="https://github.com/JuliaSmoothOptimizers/NLPModelsJuMP.jl">NLPModelsJuMP</a> implemented in <a href="https://github.com/JuliaSmoothOptimizers/OptimizationProblems.jl">OptimizationProblems</a>.</p><div><pre><code class="language-julia">using NLPModelsJuMP, OptimizationProblems

nlp = MathProgNLPModel(woods())
x, fx, ngx, optimal, iter = steepest(nlp)
println(&quot;fx = $fx&quot;)
println(&quot;ngx = $ngx&quot;)
println(&quot;optimal = $optimal&quot;)
println(&quot;iter = $iter&quot;)</code></pre><pre><code class="language-none">fx = 1.0000000000002167
ngx = 9.893253859340887e-7
optimal = true
iter = 12016</code></pre></div><p>For constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.</p><div><pre><code class="language-julia">f(x) = (x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2
x0 = [-1.2; 1.0]
lvar = [-Inf; 0.1]
uvar = [0.5; 0.5]
c(x) = [x[1] + x[2] - 2; x[1]^2 + x[2]^2]
lcon = [0.0; -Inf]
ucon = [Inf; 1.0]
nlp = ADNLPModel(f, x0, c=c, lvar=lvar, uvar=uvar, lcon=lcon, ucon=ucon)

println(&quot;cx = $(cons(nlp, nlp.meta.x0))&quot;)
println(&quot;Jx = $(jac(nlp, nlp.meta.x0))&quot;)</code></pre><pre><code class="language-none">cx = [-2.2, 2.44]
Jx = [1.0 1.0; -2.4 2.0]</code></pre></div><h2><a class="nav-anchor" id="Nonlinear-least-squares-models-1" href="#Nonlinear-least-squares-models-1">Nonlinear least squares models</a></h2><p>In addition to the general nonlinear model, we can define the residual function for a nonlinear least-squares problem. In other words, the objective function of the problem is of the form <span>$f(x) = \tfrac{1}{2}\|F(x)\|^2$</span>, and we can define the function <span>$F$</span> and its derivatives.</p><p>A simple way to define an NLS problem is with <code>ADNLSModel</code>, which uses automatic differentiation.</p><div><pre><code class="language-julia">F(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]
x0 = [-1.2; 1.0]
nls = ADNLSModel(F, x0, 2) # 2 nonlinear equations
residual(nls, x0)</code></pre><pre><code class="language-none">2-element Array{Float64,1}:
 -2.2
 -4.3999999999999995</code></pre></div><div><pre><code class="language-julia">jac_residual(nls, x0)</code></pre><pre><code class="language-none">2×2 Array{Float64,2}:
  1.0   0.0
 24.0  10.0</code></pre></div><p>We can also define a linear least squares by passing the matrices that define the problem</p><div>\[\begin{align*}
\min \quad &amp; \tfrac{1}{2}\|Ax - b\|^2 \\
&amp; c_L  \leq Cx \leq c_U \\
&amp; \ell \leq  x \leq u.
\end{align*}\]</div><div><pre><code class="language-julia">A = rand(10, 3)
b = rand(10)
C = rand(2, 3)
nls = LLSModel(A, b, C=C, lcon=zeros(2), ucon=zeros(2), lvar=-ones(3), uvar=ones(3))

@info norm(jac_residual(nls, zeros(3)) - A)
@info norm(jac(nls, zeros(3)) - C)</code></pre><pre><code class="language-none">[ Info: 0.0
[ Info: 0.0</code></pre></div><p>Another way to define a nonlinear least squares is using <code>FeasibilityResidual</code> to consider the constraints of a general nonlinear problem as the residual of the NLS.</p><div><pre><code class="language-julia">nlp = ADNLPModel(x-&gt;0, # objective doesn&#39;t matter,
                 ones(2), c=x-&gt;[x[1] + x[2] - 1; x[1] * x[2] - 2],
                 lcon=zeros(2), ucon=zeros(2))
nls = FeasibilityResidual(nlp)
s = 0.0
for t = 1:100
  global s
  x = rand(2)
  s += norm(residual(nls, x) - cons(nlp, x))
end
@info &quot;s = $s&quot;</code></pre><pre><code class="language-none">[ Info: s = 0.0</code></pre></div><footer><hr/><a class="previous" href="../tools/"><span class="direction">Previous</span><span class="title">Tools</span></a><a class="next" href="../api/"><span class="direction">Next</span><span class="title">API</span></a></footer></article></body></html>

var documenterSearchIndex = {"docs":
[{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"As stated in the Home page, we consider the nonlinear optimization problem in the following format:","category":"page"},{"location":"api/#","page":"API","title":"API","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n ell leq x leq u\nendaligned","category":"page"},{"location":"api/#","page":"API","title":"API","text":"To develop an optimization algorithm, we are usually worried not only with f(x) and c(x), but also with their derivatives. Namely,","category":"page"},{"location":"api/#","page":"API","title":"API","text":"nabla f(x), the gradient of f at the point x;\nnabla^2 f(x), the Hessian of f at the point x;\nJ(x) = nabla c(x)^T, the Jacobian of c at the point x;\nnabla^2 f(x) + sum_i=1^m lambda_i nabla^2 c_i(x), the Hessian of the Lagrangian function at the point (xlambda).","category":"page"},{"location":"api/#","page":"API","title":"API","text":"There are many ways to access some of these values, so here is a little reference guide.","category":"page"},{"location":"api/#Reference-guide-1","page":"API","title":"Reference guide","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"The following naming should be easy enough to follow. If not, click on the link and go to the description.","category":"page"},{"location":"api/#","page":"API","title":"API","text":"! means inplace;\n_coord means coordinate format;\nprod means matrix-vector product;\n_op means operator (as in LinearOperators.jl).","category":"page"},{"location":"api/#","page":"API","title":"API","text":"Feel free to open an issue to suggest other methods that should apply to all NLPModels instances.","category":"page"},{"location":"api/#","page":"API","title":"API","text":"Function NLPModels function\nf(x) obj, objgrad, objgrad!, objcons, objcons!\nnabla f(x) grad, grad!, objgrad, objgrad!\nnabla^2 f(x) hess, hess_op, hess_op!, hess_coord, hess_coord, hess_structure, hess_structure!, hprod, hprod!\nc(x) cons, cons!, objcons, objcons!\nJ(x) jac, jac_op, jac_op!, jac_coord, jac_coord!, jac_structure, jprod, jprod!, jtprod, jtprod!\nnabla^2 L(xy) hess, hess_op, hess_coord, hess_coord!, hess_structure, hess_structure!, hprod, hprod!","category":"page"},{"location":"api/#nls-api-1","page":"API","title":"API for NLSModels","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"For the Nonlinear Least Squares models, f(x) = tfrac12 Vert F(x)Vert^2, and these models have additional function to access the residual value and its derivatives. Namely,","category":"page"},{"location":"api/#","page":"API","title":"API","text":"J_F(x) = nabla F(x)^T\nnabla^2 F_i(x)","category":"page"},{"location":"api/#","page":"API","title":"API","text":"Function function\nF(x) residual, residual!\nJ_F(x) jac_residual, jac_coord_residual, jac_coord_residual!, jac_structure_residual, jprod_residual, jprod_residual!, jtprod_residual, jtprod_residual!, jac_op_residual, jac_op_residual!\nnabla^2 F_i(x) hess_residual, hess_coord_residual, hess_coord_residual!, hess_structure_residual, hess_structure_residual!, jth_hess_residual, hprod_residual, hprod_residual!, hess_op_residual, hess_op_residual!","category":"page"},{"location":"api/#AbstractNLPModel-functions-1","page":"API","title":"AbstractNLPModel functions","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"obj\ngrad\ngrad!\nobjgrad\nobjgrad!\ncons\ncons!\nobjcons\nobjcons!\njac_coord\njac_coord!\njac_structure\njac_structure!\njac\njac_op\njac_op!\njprod\njprod!\njtprod\njtprod!\nhess_coord\nhess_coord!\nhess_structure\nhess_structure!\nhess\nhess_op\nhess_op!\nhprod\nhprod!\nreset!\nreset_data!","category":"page"},{"location":"api/#NLPModels.obj","page":"API","title":"NLPModels.obj","text":"f = obj(nlp, x)\n\nEvaluate f(x), the objective function of nlp at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.grad","page":"API","title":"NLPModels.grad","text":"g = grad(nlp, x)\n\nEvaluate f(x), the gradient of the objective function at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.grad!","page":"API","title":"NLPModels.grad!","text":"g = grad!(nlp, x, g)\n\nEvaluate f(x), the gradient of the objective function at x in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.objgrad","page":"API","title":"NLPModels.objgrad","text":"f, g = objgrad(nlp, x)\n\nEvaluate f(x) and f(x) at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.objgrad!","page":"API","title":"NLPModels.objgrad!","text":"f, g = objgrad!(nlp, x, g)\n\nEvaluate f(x) and f(x) at x. g is overwritten with the value of f(x).\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.cons","page":"API","title":"NLPModels.cons","text":"c = cons(nlp, x)\n\nEvaluate c(x), the constraints at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.cons!","page":"API","title":"NLPModels.cons!","text":"c = cons!(nlp, x, c)\n\nEvaluate c(x), the constraints at x in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.objcons","page":"API","title":"NLPModels.objcons","text":"f, c = objcons(nlp, x)\n\nEvaluate f(x) and c(x) at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.objcons!","page":"API","title":"NLPModels.objcons!","text":"f = objcons!(nlp, x, c)\n\nEvaluate f(x) and c(x) at x. c is overwritten with the value of c(x).\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_coord","page":"API","title":"NLPModels.jac_coord","text":"vals = jac_coord(nlp, x)\n\nEvaluate J(x), the constraint's Jacobian at x in sparse coordinate format.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_coord!","page":"API","title":"NLPModels.jac_coord!","text":"vals = jac_coord!(nlp, x, vals)\n\nEvaluate J(x), the constraint's Jacobian at x in sparse coordinate format, rewriting vals.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_structure","page":"API","title":"NLPModels.jac_structure","text":"(rows,cols) = jac_structure(nlp)\n\nReturn the structure of the constraint's Jacobian in sparse coordinate format.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_structure!","page":"API","title":"NLPModels.jac_structure!","text":"jac_structure!(nlp, rows, cols)\n\nReturn the structure of the constraint's Jacobian in sparse coordinate format in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac","page":"API","title":"NLPModels.jac","text":"Jx = jac(nlp, x)\n\nEvaluate J(x), the constraint's Jacobian at x as a sparse matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_op","page":"API","title":"NLPModels.jac_op","text":"J = jac_op(nlp, x)\n\nReturn the Jacobian at x as a linear operator. The resulting object may be used as if it were a matrix, e.g., J * v or J' * v.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_op!","page":"API","title":"NLPModels.jac_op!","text":"J = jac_op!(nlp, x, Jv, Jtv)\n\nReturn the Jacobian at x as a linear operator. The resulting object may be used as if it were a matrix, e.g., J * v or J' * v. The values Jv and Jtv are used as preallocated storage for the operations.\n\n\n\n\n\nJ = jac_op!(nlp, rows, cols, vals, Jv, Jtv)\n\nReturn the Jacobian given by (rows, cols, vals) as a linear operator. The resulting object may be used as if it were a matrix, e.g., J * v or J' * v. The values Jv and Jtv are used as preallocated storage for the operations.\n\n\n\n\n\nJ = jac_op!(nlp, x, rows, cols, Jv, Jtv)\n\nReturn the Jacobian at x as a linear operator. The resulting object may be used as if it were a matrix, e.g., J * v or J' * v. (rows, cols) should be the sparsity structure of the Jacobian. The values Jv and Jtv are used as preallocated storage for the operations.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jprod","page":"API","title":"NLPModels.jprod","text":"Jv = jprod(nlp, x, v)\n\nEvaluate J(x)v, the Jacobian-vector product at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jprod!","page":"API","title":"NLPModels.jprod!","text":"Jv = jprod!(nlp, x, v, Jv)\n\nEvaluate J(x)v, the Jacobian-vector product at x in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jtprod","page":"API","title":"NLPModels.jtprod","text":"Jtv = jtprod(nlp, x, v, Jtv)\n\nEvaluate J(x)^Tv, the transposed-Jacobian-vector product at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jtprod!","page":"API","title":"NLPModels.jtprod!","text":"Jtv = jtprod!(nlp, x, v, Jtv)\n\nEvaluate J(x)^Tv, the transposed-Jacobian-vector product at x in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_coord","page":"API","title":"NLPModels.hess_coord","text":"vals = hess_coord(nlp, x; obj_weight=1.0)\n\nEvaluate the objective Hessian at x in sparse coordinate format, with objective function scaled by obj_weight, i.e.,\n\nσ ²f(x)\n\nwith σ = obj_weight . Only the lower triangle is returned.\n\n\n\n\n\nvals = hess_coord(nlp, x, y; obj_weight=1.0)\n\nEvaluate the Lagrangian Hessian at (x,y) in sparse coordinate format, with objective function scaled by obj_weight, i.e.,\n\n²L(xy) = σ ²f(x) + sum_i yᵢ ²cᵢ(x)\n\nwith σ = obj_weight . Only the lower triangle is returned.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_coord!","page":"API","title":"NLPModels.hess_coord!","text":"vals = hess_coord!(nlp, x, y, vals; obj_weight=1.0)\n\nEvaluate the Lagrangian Hessian at (x,y) in sparse coordinate format, with objective function scaled by obj_weight, i.e.,\n\n²L(xy) = σ ²f(x) + sum_i yᵢ ²cᵢ(x)\n\nwith σ = obj_weight , rewriting vals. Only the lower triangle is returned.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_structure","page":"API","title":"NLPModels.hess_structure","text":"(rows,cols) = hess_structure(nlp)\n\nReturn the structure of the Lagrangian Hessian in sparse coordinate format.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_structure!","page":"API","title":"NLPModels.hess_structure!","text":"hess_structure!(nlp, rows, cols)\n\nReturn the structure of the Lagrangian Hessian in sparse coordinate format in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess","page":"API","title":"NLPModels.hess","text":"Hx = hess(nlp, x; obj_weight=1.0)\n\nEvaluate the objective Hessian at x as a sparse matrix, with objective function scaled by obj_weight, i.e.,\n\nσ ²f(x)\n\nwith σ = obj_weight . Only the lower triangle is returned.\n\n\n\n\n\nHx = hess(nlp, x, y; obj_weight=1.0)\n\nEvaluate the Lagrangian Hessian at (x,y) as a sparse matrix, with objective function scaled by obj_weight, i.e.,\n\n²L(xy) = σ ²f(x) + sum_i yᵢ ²cᵢ(x)\n\nwith σ = obj_weight . Only the lower triangle is returned.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_op","page":"API","title":"NLPModels.hess_op","text":"H = hess_op(nlp, x; obj_weight=1.0)\n\nReturn the objective Hessian at x with objective function scaled by obj_weight as a linear operator. The resulting object may be used as if it were a matrix, e.g., H * v. The linear operator H represents\n\nσ ²f(x)\n\nwith σ = obj_weight .\n\n\n\n\n\nH = hess_op(nlp, x, y; obj_weight=1.0)\n\nReturn the Lagrangian Hessian at (x,y) with objective function scaled by obj_weight as a linear operator. The resulting object may be used as if it were a matrix, e.g., H * v. The linear operator H represents\n\n²L(xy) = σ ²f(x) + sum_i yᵢ ²cᵢ(x)\n\nwith σ = obj_weight .\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_op!","page":"API","title":"NLPModels.hess_op!","text":"H = hess_op!(nlp, x, Hv; obj_weight=1.0)\n\nReturn the objective Hessian at x with objective function scaled by obj_weight as a linear operator, and storing the result on Hv. The resulting object may be used as if it were a matrix, e.g., w = H * v. The vector Hv is used as preallocated storage for the operation.  The linear operator H represents\n\nσ ²f(x)\n\nwith σ = obj_weight .\n\n\n\n\n\nH = hess_op!(nlp, rows, cols, vals, Hv)\n\nReturn the Hessian given by (rows, cols, vals) as a linear operator, and storing the result on Hv. The resulting object may be used as if it were a matrix, e.g., w = H * v.   The vector Hv is used as preallocated storage for the operation.  The linear operator H represents\n\nσ ²f(x)\n\nwith σ = obj_weight .\n\n\n\n\n\nH = hess_op!(nlp, x, rows, cols, Hv; obj_weight=1.0)\n\nReturn the objective Hessian at x with objective function scaled by obj_weight as a linear operator, and storing the result on Hv. The resulting object may be used as if it were a matrix, e.g., w = H * v. (rows, cols) should be the sparsity structure of the Hessian. The vector Hv is used as preallocated storage for the operation.  The linear operator H represents\n\nσ ²f(x)\n\nwith σ = obj_weight .\n\n\n\n\n\nH = hess_op!(nlp, x, y, Hv; obj_weight=1.0)\n\nReturn the Lagrangian Hessian at (x,y) with objective function scaled by obj_weight as a linear operator, and storing the result on Hv. The resulting object may be used as if it were a matrix, e.g., w = H * v. The vector Hv is used as preallocated storage for the operation.  The linear operator H represents\n\n²L(xy) = σ ²f(x) + sum_i yᵢ ²cᵢ(x)\n\nwith σ = obj_weight .\n\n\n\n\n\nH = hess_op!(nlp, x, y, rows, cols, Hv; obj_weight=1.0)\n\nReturn the Lagrangian Hessian at (x,y) with objective function scaled by obj_weight as a linear operator, and storing the result on Hv. The resulting object may be used as if it were a matrix, e.g., w = H * v. (rows, cols) should be the sparsity structure of the Hessian. The vector Hv is used as preallocated storage for the operation.  The linear operator H represents\n\nσ ²f(x)\n\nwith σ = obj_weight .\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hprod","page":"API","title":"NLPModels.hprod","text":"Hv = hprod(nlp, x, v; obj_weight=1.0)\n\nEvaluate the product of the objective Hessian at x with the vector v, with objective function scaled by obj_weight, where the objective Hessian is\n\nσ ²f(x)\n\nwith σ = obj_weight .\n\n\n\n\n\nHv = hprod(nlp, x, y, v; obj_weight=1.0)\n\nEvaluate the product of the Lagrangian Hessian at (x,y) with the vector v, with objective function scaled by obj_weight, where the Lagrangian Hessian is\n\n²L(xy) = σ ²f(x) + sum_i yᵢ ²cᵢ(x)\n\nwith σ = obj_weight .\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hprod!","page":"API","title":"NLPModels.hprod!","text":"Hv = hprod!(nlp, x, y, v, Hv; obj_weight=1.0)\n\nEvaluate the product of the Lagrangian Hessian at (x,y) with the vector v in place, with objective function scaled by obj_weight, where the Lagrangian Hessian is\n\n²L(xy) = σ ²f(x) + sum_i yᵢ ²cᵢ(x)\n\nwith σ = obj_weight .\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearOperators.reset!","page":"API","title":"LinearOperators.reset!","text":"reset!(counters)\n\nReset evaluation counters\n\n\n\n\n\nreset!(nlp)\n\nReset evaluation count in nlp\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.reset_data!","page":"API","title":"NLPModels.reset_data!","text":"reset_data!(nlp)\n\nReset model data if appropriate. This method should be overloaded if a subtype of AbstractNLPModel contains data that should be reset, such as a quasi-Newton linear operator.\n\n\n\n\n\n","category":"function"},{"location":"api/#AbstractNLSModel-1","page":"API","title":"AbstractNLSModel","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"residual\nresidual!\njac_residual\njac_coord_residual\njac_coord_residual!\njac_structure_residual\njac_structure_residual!\njprod_residual\njprod_residual!\njtprod_residual\njtprod_residual!\njac_op_residual\njac_op_residual!\nhess_residual\nhess_coord_residual\nhess_coord_residual!\nhess_structure_residual\nhess_structure_residual!\njth_hess_residual\nhprod_residual\nhprod_residual!\nhess_op_residual\nhess_op_residual!","category":"page"},{"location":"api/#NLPModels.residual","page":"API","title":"NLPModels.residual","text":"Fx = residual(nls, x)\n\nComputes F(x), the residual at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.residual!","page":"API","title":"NLPModels.residual!","text":"Fx = residual!(nls, x, Fx)\n\nComputes F(x), the residual at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_residual","page":"API","title":"NLPModels.jac_residual","text":"Jx = jac_residual(nls, x)\n\nComputes J(x), the Jacobian of the residual at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_coord_residual","page":"API","title":"NLPModels.jac_coord_residual","text":"(rows,cols,vals) = jac_coord_residual(nls, x)\n\nComputes the Jacobian of the residual at x in sparse coordinate format.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_coord_residual!","page":"API","title":"NLPModels.jac_coord_residual!","text":"vals = jac_coord_residual!(nls, x, vals)\n\nComputes the Jacobian of the residual at x in sparse coordinate format, rewriting vals. rows and cols are not rewritten.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_structure_residual","page":"API","title":"NLPModels.jac_structure_residual","text":"(rows,cols) = jac_structure_residual(nls)\n\nReturns the structure of the constraint's Jacobian in sparse coordinate format.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_structure_residual!","page":"API","title":"NLPModels.jac_structure_residual!","text":"(rows,cols) = jac_structure_residual!(nls, rows, cols)\n\nReturns the structure of the constraint's Jacobian in sparse coordinate format in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jprod_residual","page":"API","title":"NLPModels.jprod_residual","text":"Jv = jprod_residual(nls, x, v)\n\nComputes the product of the Jacobian of the residual at x and a vector, i.e.,  J(x)v.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jprod_residual!","page":"API","title":"NLPModels.jprod_residual!","text":"Jv = jprod_residual!(nls, x, v, Jv)\n\nComputes the product of the Jacobian of the residual at x and a vector, i.e.,  J(x)v, storing it in Jv.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jtprod_residual","page":"API","title":"NLPModels.jtprod_residual","text":"Jtv = jtprod_residual(nls, x, v)\n\nComputes the product of the transpose of the Jacobian of the residual at x and a vector, i.e.,  J(x)^Tv.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jtprod_residual!","page":"API","title":"NLPModels.jtprod_residual!","text":"Jtv = jtprod_residual!(nls, x, v, Jtv)\n\nComputes the product of the transpose of the Jacobian of the residual at x and a vector, i.e.,  J(x)^Tv, storing it in Jtv.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_op_residual","page":"API","title":"NLPModels.jac_op_residual","text":"Jx = jac_op_residual(nls, x)\n\nComputes J(x), the Jacobian of the residual at x, in linear operator form.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jac_op_residual!","page":"API","title":"NLPModels.jac_op_residual!","text":"Jx = jac_op_residual!(nls, x, Jv, Jtv)\n\nComputes J(x), the Jacobian of the residual at x, in linear operator form. The vectors Jv and Jtv are used as preallocated storage for the operations.\n\n\n\n\n\nJx = jac_op_residual!(nls, rows, cols, vals, Jv, Jtv)\n\nComputes J(x), the Jacobian of the residual given by (rows, cols, vals), in linear operator form. The vectors Jv and Jtv are used as preallocated storage for the operations.\n\n\n\n\n\nJx = jac_op_residual!(nls, x, rows, cols, Jv, Jtv)\n\nComputes J(x), the Jacobian of the residual at x, in linear operator form. The vectors Jv and Jtv are used as preallocated storage for the operations. The structure of the Jacobian should be given by (rows, cols).\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_residual","page":"API","title":"NLPModels.hess_residual","text":"H = hess_residual(nls, x, v)\n\nComputes the linear combination of the Hessians of the residuals at x with coefficients v.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_coord_residual","page":"API","title":"NLPModels.hess_coord_residual","text":"vals = hess_coord_residual(nls, x, v)\n\nComputes the linear combination of the Hessians of the residuals at x with coefficients v in sparse coordinate format.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_coord_residual!","page":"API","title":"NLPModels.hess_coord_residual!","text":"vals = hess_coord_residual!(nls, x, v, vals)\n\nComputes the linear combination of the Hessians of the residuals at x with coefficients v in sparse coordinate format, rewriting vals.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_structure_residual","page":"API","title":"NLPModels.hess_structure_residual","text":"(rows,cols) = hess_structure_residual(nls)\n\nReturns the structure of the residual Hessian.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_structure_residual!","page":"API","title":"NLPModels.hess_structure_residual!","text":"hess_structure_residual!(nls, rows, cols)\n\nReturns the structure of the residual Hessian in place.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jth_hess_residual","page":"API","title":"NLPModels.jth_hess_residual","text":"Hj = jth_hess_residual(nls, x, j)\n\nComputes the Hessian of the j-th residual at x.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hprod_residual","page":"API","title":"NLPModels.hprod_residual","text":"Hiv = hprod_residual(nls, x, i, v)\n\nComputes the product of the Hessian of the i-th residual at x, times the vector v.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hprod_residual!","page":"API","title":"NLPModels.hprod_residual!","text":"Hiv = hprod_residual!(nls, x, i, v, Hiv)\n\nComputes the product of the Hessian of the i-th residual at x, times the vector v, and stores it in vector Hiv.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_op_residual","page":"API","title":"NLPModels.hess_op_residual","text":"Hop = hess_op_residual(nls, x, i)\n\nComputes the Hessian of the i-th residual at x, in linear operator form.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hess_op_residual!","page":"API","title":"NLPModels.hess_op_residual!","text":"Hop = hess_op_residual!(nls, x, i, Hiv)\n\nComputes the Hessian of the i-th residual at x, in linear operator form. The vector Hiv is used as preallocated storage for the operation.\n\n\n\n\n\n","category":"function"},{"location":"api/#Derivative-Checker-1","page":"API","title":"Derivative Checker","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"gradient_check\njacobian_check\nhessian_check\nhessian_check_from_grad","category":"page"},{"location":"api/#NLPModels.gradient_check","page":"API","title":"NLPModels.gradient_check","text":"gradient_check(nlp; x=nlp.meta.x0, atol=1e-6, rtol=1e-4)\n\nCheck the first derivatives of the objective at x against centered finite differences.\n\nThis function returns a dictionary indexed by components of the gradient for which the relative error exceeds rtol.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.jacobian_check","page":"API","title":"NLPModels.jacobian_check","text":"jacobian_check(nlp; x=nlp.meta.x0, atol=1e-6, rtol=1e-4)\n\nCheck the first derivatives of the constraints at x against centered finite differences.\n\nThis function returns a dictionary indexed by (j, i) tuples such that the relative error in the i-th partial derivative of the j-th constraint exceeds rtol.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hessian_check","page":"API","title":"NLPModels.hessian_check","text":"hessian_check(nlp; x=nlp.meta.x0, atol=1e-6, rtol=1e-4, sgn=1)\n\nCheck the second derivatives of the objective and each constraints at x against centered finite differences. This check does not rely on exactness of the first derivatives, only on objective and constraint values.\n\nThe sgn arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as\n\nL(xy) = f(x) + sum_j yⱼ cⱼ(x)\n\nand a negative value if the Lagrangian is formulated as\n\nL(xy) = f(x) - sum_j yⱼ cⱼ(x)\n\nOnly the sign of sgn is important.\n\nThis function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k > 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative ∂²fₖ/∂xᵢ∂xⱼ exceeds rtol.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.hessian_check_from_grad","page":"API","title":"NLPModels.hessian_check_from_grad","text":"hessian_check_from_grad(nlp; x=nlp.meta.x0, atol=1e-6, rtol=1e-4, sgn=1)\n\nCheck the second derivatives of the objective and each constraints at x against centered finite differences. This check assumes exactness of the first derivatives.\n\nThe sgn arguments refers to the formulation of the Lagrangian in the problem. It should have a positive value if the Lagrangian is formulated as\n\nL(xy) = f(x) + sum_j yⱼ cⱼ(x)\n\nand a negative value if the Lagrangian is formulated as\n\nL(xy) = f(x) - sum_j yⱼ cⱼ(x)\n\nOnly the sign of sgn is important.\n\nThis function returns a dictionary indexed by functions. The 0-th function is the objective while the k-th function (for k > 0) is the k-th constraint. The values of the dictionary are dictionaries indexed by tuples (i, j) such that the relative error in the second derivative ∂²fₖ/∂xᵢ∂xⱼ exceeds rtol.\n\n\n\n\n\n","category":"function"},{"location":"api/#Internal-1","page":"API","title":"Internal","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"coo_prod!\ncoo_sym_prod!\n@default_counters\n@default_nlscounters\nNLPModels.increment!\nNLPModels.decrement!","category":"page"},{"location":"api/#NLPModels.coo_prod!","page":"API","title":"NLPModels.coo_prod!","text":"coo_prod!(rows, cols, vals, v, Av)\n\nCompute the product of a matrix A given by (rows, cols, vals) and the vector v. The result is stored in Av, which should have length equals to the number of rows of A.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.coo_sym_prod!","page":"API","title":"NLPModels.coo_sym_prod!","text":"coo_sym_prod!(rows, cols, vals, v, Av)\n\nCompute the product of a symmetric matrix A given by (rows, cols, vals) and the vector v. The result is stored in Av, which should have length equals to the number of rows of A. Only one triangle of A should be passed.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.@default_counters","page":"API","title":"NLPModels.@default_counters","text":"@default_counters Model inner\n\nDefine functions relating counters of Model to counters of Model.inner.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NLPModels.@default_nlscounters","page":"API","title":"NLPModels.@default_nlscounters","text":"@default_nlscounters Model inner\n\nDefine functions relating NLS counters of Model to NLS counters of Model.inner.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NLPModels.increment!","page":"API","title":"NLPModels.increment!","text":"increment!(nlp, s)\n\nIncrement counter s of problem nlp.\n\n\n\n\n\n","category":"function"},{"location":"api/#NLPModels.decrement!","page":"API","title":"NLPModels.decrement!","text":"decrement!(nlp, s)\n\nDecrement counter s of problem nlp.\n\n\n\n\n\n","category":"function"},{"location":"models/#Models-1","page":"Models","title":"Models","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"The following general models are implemented in this package:","category":"page"},{"location":"models/#","page":"Models","title":"Models","text":"ADNLPModel\nDerived Models\nSlackModel\nLBFGSModel\nLSR1Model","category":"page"},{"location":"models/#","page":"Models","title":"Models","text":"In addition, the following nonlinear least squares models are implemented in this package:","category":"page"},{"location":"models/#","page":"Models","title":"Models","text":"ADNLSModel\nFeasibilityResidual\nLLSModel\nSlackNLSModel\nFeasibilityFormNLS","category":"page"},{"location":"models/#","page":"Models","title":"Models","text":"There are other external models implemented. In particular,","category":"page"},{"location":"models/#","page":"Models","title":"Models","text":"AmplModel\nCUTEstModel\nMathProgNLPModel and MathProgNLSModel using JuMP/MPB.","category":"page"},{"location":"models/#","page":"Models","title":"Models","text":"There are currently two models implemented in this package, besides the external ones.","category":"page"},{"location":"models/#NLPModels-1","page":"Models","title":"NLPModels","text":"","category":"section"},{"location":"models/#ADNLPModel-1","page":"Models","title":"ADNLPModel","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.ADNLPModel","category":"page"},{"location":"models/#NLPModels.ADNLPModel","page":"Models","title":"NLPModels.ADNLPModel","text":"ADNLPModel(f, x0)\nADNLPModel(f, x0, lvar, uvar)\nADNLPModel(f, x0, c, lcon, ucon)\nADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\n\nADNLPModel is an AbstractNLPModel using ForwardDiff to compute the derivatives. The problem is defined as\n\n min  f(x)\ns.to  lcon ≤ c(x) ≤ ucon\n      lvar ≤   x  ≤ uvar.\n\nThe following keyword arguments are available to all constructors:\n\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\nlin: An array of indexes of the linear constraints (default: Int[])\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\n\n\n\n\n","category":"type"},{"location":"models/#Example-1","page":"Models","title":"Example","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"using NLPModels\nf(x) = sum(x.^4)\nx = [1.0; 0.5; 0.25; 0.125]\nnlp = ADNLPModel(f, x)\ngrad(nlp, x)","category":"page"},{"location":"models/#Derived-Models-1","page":"Models","title":"Derived Models","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"The following models are created from any given model, making some modification to that model.","category":"page"},{"location":"models/#SlackModel-1","page":"Models","title":"SlackModel","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.SlackModel","category":"page"},{"location":"models/#NLPModels.SlackModel","page":"Models","title":"NLPModels.SlackModel","text":"A model whose only inequality constraints are bounds.\n\nGiven a model, this type represents a second model in which slack variables are introduced so as to convert linear and nonlinear inequality constraints to equality constraints and bounds. More precisely, if the original model has the form\n\nbeginaligned\n       min_x quad  f(x)\nmathrmst quad  c_L  c(x)  c_U\n                      ℓ    x     u\nendaligned\n\nthe new model appears to the user as\n\nbeginaligned\n       min_X quad  f(X)\nmathrmst quad  g(X) = 0\n                     L  X  U\nendaligned\n\nThe unknowns X = (x s) contain the original variables and slack variables s. The latter are such that the new model has the general form\n\nbeginaligned\n       min_x quad  f(x)\nmathrmst quad  c(x) - s = 0\n                     c_L  s  c_U\n                      ℓ   x  u\nendaligned\n\nalthough no slack variables are introduced for equality constraints.\n\nThe slack variables are implicitly ordered as [s(low), s(upp), s(rng)], where low, upp and rng represent the indices of the constraints of the form c_L  c(x)  , -  c(x)  c_U and c_L  c(x)  c_U, respectively.\n\n\n\n\n\n","category":"type"},{"location":"models/#Example-2","page":"Models","title":"Example","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"using NLPModels\nf(x) = x[1]^2 + 4x[2]^2\nc(x) = [x[1]*x[2] - 1]\nx = [2.0; 2.0]\nnlp = ADNLPModel(f, x, c, [0.0], [0.0])\nnlp_slack = SlackModel(nlp)\nnlp_slack.meta.lvar","category":"page"},{"location":"models/#LBFGSModel-1","page":"Models","title":"LBFGSModel","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.LBFGSModel","category":"page"},{"location":"models/#NLPModels.LBFGSModel","page":"Models","title":"NLPModels.LBFGSModel","text":"Construct a LBFGSModel from another type of model.\n\n\n\n\n\n","category":"type"},{"location":"models/#LSR1Model-1","page":"Models","title":"LSR1Model","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.LSR1Model","category":"page"},{"location":"models/#NLPModels.LSR1Model","page":"Models","title":"NLPModels.LSR1Model","text":"Construct a LSR1Model from another type of nlp.\n\n\n\n\n\n","category":"type"},{"location":"models/#NLSModels-1","page":"Models","title":"NLSModels","text":"","category":"section"},{"location":"models/#ADNLSModel-1","page":"Models","title":"ADNLSModel","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.ADNLSModel","category":"page"},{"location":"models/#NLPModels.ADNLSModel","page":"Models","title":"NLPModels.ADNLSModel","text":"ADNLSModel(F, x0, nequ)\nADNLSModel(F, x0, nequ, lvar, uvar)\nADNLSModel(F, x0, nequ, c, lcon, ucon)\nADNLSModel(F, x0, nequ, lvar, uvar, c, lcon, ucon)\n\nADNLSModel is an Nonlinear Least Squares model using ForwardDiff to compute the derivatives. The problem is defined as\n\n min  ½‖F(x)‖²\ns.to  lcon ≤ c(x) ≤ ucon\n      lvar ≤   x  ≤ uvar\n\nThe following keyword arguments are available to all constructors:\n\nlinequ: An array of indexes of the linear equations (default: Int[])\nname: The name of the model (default: \"Generic\")\n\nThe following keyword arguments are available to the constructors for constrained problems:\n\nlin: An array of indexes of the linear constraints (default: Int[])\ny0: An inital estimate to the Lagrangian multipliers (default: zeros)\n\n\n\n\n\n","category":"type"},{"location":"models/#","page":"Models","title":"Models","text":"using NLPModels\nF(x) = [x[1] - 1; 10*(x[2] - x[1]^2)]\nnlp = ADNLSModel(F, [-1.2; 1.0], 2)\nresidual(nlp, nlp.meta.x0)","category":"page"},{"location":"models/#FeasibilityResidual-1","page":"Models","title":"FeasibilityResidual","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.FeasibilityResidual","category":"page"},{"location":"models/#NLPModels.FeasibilityResidual","page":"Models","title":"NLPModels.FeasibilityResidual","text":"A feasibility residual model is created from a NLPModel of the form\n\nbeginaligned\n       min_x quad  f(x) \nmathrmst quad  c_L  c(x)  c_U \n                       ell    x   u\nendaligned\n\nby creating slack variables s = c(x) and defining an NLS problem from the equality constraints. The resulting problem is a bound-constrained nonlinear least-squares problem with residual function F(xs) = c(x) - s:\n\nbeginaligned\n       min_x quad  tfrac12 c(x) - s^2 \nmathrmst quad  ell  x  u \n                       c_L  s  c_U\nendaligned\n\nNotice that this problem is an AbstractNLSModel, thus the residual value, Jacobian and Hessian are explicitly defined through the NLS API. The slack variables are created using SlackModel. If ell_i = u_i, no slack variable is created. In particular, if there are only equality constrained of the form c(x) = 0, the resulting NLS is simply min_x tfrac12c(x)^2.\n\n\n\n\n\n","category":"type"},{"location":"models/#LLSModel-1","page":"Models","title":"LLSModel","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.LLSModel","category":"page"},{"location":"models/#NLPModels.LLSModel","page":"Models","title":"NLPModels.LLSModel","text":"nls = LLSModel(A, b; lvar, uvar, C, lcon, ucon)\n\nCreates a Linear Least Squares model tfrac12Ax - b^2 with optional bounds lvar ≦ x ≦ uvar and optional linear constraints lcon ≦ Cx ≦ ucon. This problem is a nonlinear least-squares problem with residual given by F(x) = Ax - b.\n\n\n\n\n\n","category":"type"},{"location":"models/#SlackNLSModel-1","page":"Models","title":"SlackNLSModel","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.SlackNLSModel","category":"page"},{"location":"models/#NLPModels.SlackNLSModel","page":"Models","title":"NLPModels.SlackNLSModel","text":"Like SlackModel, this model converts inequalities into equalities and bounds.\n\n\n\n\n\n","category":"type"},{"location":"models/#FeasibilityFormNLS-1","page":"Models","title":"FeasibilityFormNLS","text":"","category":"section"},{"location":"models/#","page":"Models","title":"Models","text":"NLPModels.FeasibilityFormNLS","category":"page"},{"location":"models/#NLPModels.FeasibilityFormNLS","page":"Models","title":"NLPModels.FeasibilityFormNLS","text":"Converts a nonlinear least-squares problem with residual F(x) to a nonlinear optimization problem with constraints F(x) = r and objective tfrac12r^2. In other words, converts\n\nbeginaligned\n       min_x quad  tfrac12F(x)^2 \nmathrmst quad  c_L  c(x)  c_U \n                         ℓ    x   u\nendaligned\n\nto\n\nbeginaligned\n   min_xr quad  tfrac12r^2 \nmathrmst quad  F(x) - r = 0 \n                       c_L  c(x)  c_U \n                         ℓ    x   u\nendaligned\n\nIf you rather have the first problem, the nls model already works as an NLPModel of that format.\n\n\n\n\n\n","category":"type"},{"location":"guidelines/#Guidelines-1","page":"Guidelines","title":"Guidelines for creating models","text":"","category":"section"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"These are guidelines for the creation of models using NLPModels to help keeping the models uniform, and for future reference in the creation of solvers.","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"Table of contents:","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"Bare minimum\nExpected behaviour\nAdvanced counters\nAdvanced tests","category":"page"},{"location":"guidelines/#bare-minimum-1","page":"Guidelines","title":"Bare minimum","text":"","category":"section"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"Your model should derive from AbstractNLPModel or some other abstract class derived from it. It is mandatory that it have a meta :: NLPModelMeta field, storing all the relevant problem information. The model also needs to provide Counters information. The easiest way is to define counters :: Counters. For instance:","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"mutable struct MyModel <: AbstractNLPModel\n  meta :: NLPModelMeta\n  counters :: Counters\nend","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"For alternatives to storing Counters in the model, check advanced counters. The minimum information that should be set for your model through NLPModelMeta is nvar, the number of variables. The following is a valid constructor for MyModel:","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"function MyModel()\n  return MyModel(NLPModelMeta(5), Counters())\nend","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"More information can be passed to NLPModelMeta. See the full list here. The essential fields are","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"x0: Starting point (defaults to zeros)\nlvar, uvar: Bounds on the variables (default to (-∞,∞))\nncon: Number of constraints (defaults to 0)\nlcon, ucon: Bounds on the constraints (default to (-∞,∞))\nnnzh: The length of the vectors used to store a triangle of the Hessian in triplet format (defaults to nvar * (nvar + 1) / 2\nnnzj: The length of the vectors used to store the Jacobian in triplet format (default to nvar * ncon)","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"There are about 30 functions in the NLPModels API, and a few with more than one signature. Luckily, many have a default implementation. We collect here the list of functions that should be implemented for a complete API.","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"Here, the following notation apply:","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"nlp is your instance of MyModel <: AbstractNLPModel\nx is the point where the function is evaluated\ny is the vector of Lagrange multipliers (for constrained problems only)\ng is the gradient vector\nH is the Hessian of the objective or Lagrangian\nhrows, hcols, and hvals are vectors storing the triplet form of the Hessian\nc is the vector of constraints\nJ is the Jacobian of the constraints\njrows, jcols, and jvals are vectors storing the triplet form of the Jacobian\nv is a vector of appropriate dimensions, generally used for operator-vector products\nJv, Jtv, Hv are vectors of appropriate dimensions, storing the result of operator-vector products","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"The following functions should be defined:","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"Objective (unconstrained models only need to worry about these)\nobj(nlp, x)\ngrad!(nlp, x, g)\nhess_structure!(nlp, hrows, hcols)\nhess_coord!(nlp, x, hvals; obj_weight=1)\nhprod!(nlp, x, v, Hv; obj_weight=1) (actually defaults to calling the constrained case)\nConstraints (constrained models need to worry about these and the ones above)\ncons!(nlp, x, c)\njac_structure!(nlp, jrows, jcols)\njac_coord!(nlp, x, jvals)\njprod!(nlp, x, v, Jv)\njtprod!(nlp, x, v, Jtv)\nhess_coord!(nlp, x, y, hvals; obj_weight=1)\nhprod!(nlp, x, y, v, Hv; obj_weight=1)","category":"page"},{"location":"guidelines/#expected-behaviour-1","page":"Guidelines","title":"Expected behaviour","text":"","category":"section"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"The following is a non-exhaustive list of expected behaviour for methods.","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"All in place methods should also return the modified vectors.\nVector inputs should have the correct size. If necessary, the user should pass them using views or slices.\nThe triplet format does not assume order nor uniqueness.","category":"page"},{"location":"guidelines/#advanced-counters-1","page":"Guidelines","title":"Advanced counters","text":"","category":"section"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"If a model does not implement counters, then it needs to define","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"neval_xxx(nlp) - get field xxx of Counters\nreset!(nlp) - resetting all counters\nincrement!(nlp, s) - increment counter s","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"For instance","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"for counter in fieldnames(Counters)\n  @eval begin\n    $counter(nlp :: MyModel) = SOMETHING\n  end\nend\nfunction reset!(nlp :: MyModel)\n  RESET COUNTERS\nend\nfunction increment!(nlp :: MyModel, s :: Symbol)\n  INCREMENT COUNTER s\nend","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"One example of such model is the SlackModel, which stores an internal model :: AbstractNLPModel, thus defining","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"$counter(nlp :: SlackModel) = $counter(nlp.model)\nreset!(nlp :: SlackModel) = reset!(nlp.model)\nincrement!(nlp :: SlackModel, s :: Symbol) = increment!(nlp.model, s)","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"This construction can be replicated calling the macro @default_counters Model inner. In the case of SlackModel, the equivalent call is","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"@default_counters SlackModel model","category":"page"},{"location":"guidelines/#advanced-tests-1","page":"Guidelines","title":"Advanced tests","text":"","category":"section"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"To test your model, in addition to writing specific test functions, it is also advised to write consistency checks. If your model can implement general problems, you can use the 6 problems in our test/problems folder implemented both as ADNLPModel and by explicitly defining these problem as models. These can be used to verify that the implementation of your model is correct through the consistent_nlps function. The simplest way to use these would be something like","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"for problem in [\"BROWNDEN\", \"HS5\", \"HS6\", \"HS10\", \"HS11\", \"HS14\"]\n  @printf(\"Checking problem %-20s\", problem)\n  nlp_ad = eval(Meta.parse(lowercase(problem) * \"_autodiff\"))() # e.g. hs5_autodiff()\n  nlp_man = eval(Meta.parse(problem))() # e.g. HS5()\n  nlp_your = ...\n  nlps = [nlp_ad, nlp_man, nlp_your]\n  consistent_nlps(nlps)\nend","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"Models with specific purposes can make use of the consistency checks by defining equivalent problems with ADNLPModel and testing them. For instance, the following model is a regularization model defined by an existing model inner, a regularization parameter ρ, and a fixed point z:","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"mutable struct RegNLP <: AbstractNLPModel\n  meta :: NLPModelMeta\n  inner :: AbstractNLPModel\n  ρ\n  z\nend","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"Assuming that all unconstrained functions are defined, the following tests will make sure that RegNLP is consistent with a specific ADNLPModel.","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"include(joinpath(dirname(pathof(NLPModels)), \"..\", \"test\", \"consistency.jl\"))\n\nf(x) = (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2\nnlp = ADNLPModel(f, [-1.2; 1.0])\nρ = rand()\nz = rand(2)\nrnlp = RegNLP(nlp, ρ, z)\nmanual = ADNLPModel(x -> f(x) + ρ * norm(x - z)^2 / 2, [-1.2; 1.0])\n\nconsistent_nlps([rnlp, manual])","category":"page"},{"location":"guidelines/#","page":"Guidelines","title":"Guidelines","text":"The complete example is available in the repository RegularizationModel.jl.","category":"page"},{"location":"reference/#Reference-1","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"#Home-1","page":"Home","title":"NLPModels.jl documentation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides general guidelines to represent optimization problems in Julia and a standardized API to evaluate the functions and their derivatives. The main objective is to be able to rely on that API when designing optimization solvers in Julia.","category":"page"},{"location":"#Introduction-1","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The general form of the optimization problem is","category":"page"},{"location":"#","page":"Home","title":"Home","text":"beginaligned\nmin quad  f(x) \n c_i(x) = 0 quad i in E \n c_L_i leq c_i(x) leq c_U_i quad i in I \n ell leq x leq u\nendaligned","category":"page"},{"location":"#","page":"Home","title":"Home","text":"where fmathbbR^nrightarrowmathbbR, cmathbbR^nrightarrowmathbbR^m, Ecup I = 12dotsm, Ecap I = emptyset, and c_L_i c_U_i ell_j u_j in mathbbRcuppminfty for i = 1dotsm and j = 1dotsn.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"For computational reasons, we write","category":"page"},{"location":"#","page":"Home","title":"Home","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n ell leq x leq u\nendaligned","category":"page"},{"location":"#","page":"Home","title":"Home","text":"defining c_L_i = c_U_i for all i in E. The Lagrangian of this problem is defined as","category":"page"},{"location":"#","page":"Home","title":"Home","text":"L(xlambdaz^Lz^Usigma) = sigma f(x) + c(x)^Tlambda  + sum_i=1^n z_i^L(x_i-l_i) + sum_i=1^nz_i^U(u_i-x_i)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"where sigma is a scaling parameter included for computational reasons. Notice that, for the Hessian, the variables z^L and z^U are not used.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Optimization problems are represented by an instance/subtype of AbstractNLPModel. Such instances are composed of","category":"page"},{"location":"#","page":"Home","title":"Home","text":"an instance of NLPModelMeta, which provides information about the problem, including the number of variables, constraints, bounds on the variables, etc.\nother data specific to the provenance of the problem.","category":"page"},{"location":"#Nonlinear-Least-Squares-1","page":"Home","title":"Nonlinear Least Squares","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"A special type of NLPModels are the NLSModels, i.e., Nonlinear Least Squares models. In these problems, the function f(x) is given by tfrac12Vert F(x)Vert^2, where F is referred as the residual function. The individual value of F, as well as of its derivatives, is also available.","category":"page"},{"location":"#Tools-1","page":"Home","title":"Tools","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"There are a few tools to use on NLPModels, for instance to query whether the problem is constrained or not, and to get the number of function evaluations. See Tools.","category":"page"},{"location":"#Install-1","page":"Home","title":"Install","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Install NLPModels.jl with the following command.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"pkg> add NLPModels","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This will enable a simple model and a model with automatic differentiation using ForwardDiff. For models using JuMP see NLPModelsJuMP.jl.","category":"page"},{"location":"#Usage-1","page":"Home","title":"Usage","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"See the Models, the Tools, the Tutorial, or the API.","category":"page"},{"location":"#Internal-Interfaces-1","page":"Home","title":"Internal Interfaces","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"ADNLPModel: Uses ForwardDiff to compute the derivatives. It has a very simple interface, though it isn't very efficient for larger problems.\nSlackModel: Creates an equality constrained problem with bounds  on the variables using an existing NLPModel.\nLBFGSModel: Creates a model using a LBFGS approximation to the Hessian using an existing NLPModel.\nLSR1Model: Creates a model using a LSR1 approximation to the Hessian using an existing NLPModel.\nADNLSModel: Similar to ADNLPModel, but for nonlinear least squares.\nFeasibilityResidual: Creates a nonlinear least squares model from an equality constrained problem in which the residual function is the constraints function.\nLLSModel: Creates a linear least squares model.\nSlackNLSModel: Creates an equality constrained nonlinear least squares problem with bounds on the variables using an existing NLSModel.\nFeasibilityFormNLS: Creates residual variables and constraints, so that the residual is linear.","category":"page"},{"location":"#External-Interfaces-1","page":"Home","title":"External Interfaces","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"AmplModel: Defined in AmplNLReader.jl for problems modeled using AMPL\nCUTEstModel: Defined in CUTEst.jl for problems from CUTEst.\nMathProgNLPModel: Uses a MathProgModel, derived from a AbstractMathProgModel model. For instance, JuMP.jl models can be used.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"If you want your interface here, open a PR.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"If you want to create your own interface, check these Guidelines.","category":"page"},{"location":"#Attributes-1","page":"Home","title":"Attributes","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"NLPModelMeta objects have the following attributes:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Attribute Type Notes\nnvar Int number of variables\nx0 Array{Float64,1} initial guess\nlvar Array{Float64,1} vector of lower bounds\nuvar Array{Float64,1} vector of upper bounds\nifix Array{Int64,1} indices of fixed variables\nilow Array{Int64,1} indices of variables with lower bound only\niupp Array{Int64,1} indices of variables with upper bound only\nirng Array{Int64,1} indices of variables with lower and upper bound (range)\nifree Array{Int64,1} indices of free variables\niinf Array{Int64,1} indices of visibly infeasible bounds\nncon Int total number of general constraints\nnlin Int number of linear constraints\nnnln Int number of nonlinear general constraints\nnnet Int number of nonlinear network constraints\ny0 Array{Float64,1} initial Lagrange multipliers\nlcon Array{Float64,1} vector of constraint lower bounds\nucon Array{Float64,1} vector of constraint upper bounds\nlin Range1{Int64} indices of linear constraints\nnln Range1{Int64} indices of nonlinear constraints (not network)\nnnet Range1{Int64} indices of nonlinear network constraints\njfix Array{Int64,1} indices of equality constraints\njlow Array{Int64,1} indices of constraints of the form c(x) ≥ cl\njupp Array{Int64,1} indices of constraints of the form c(x) ≤ cu\njrng Array{Int64,1} indices of constraints of the form cl ≤ c(x) ≤ cu\njfree Array{Int64,1} indices of \"free\" constraints (there shouldn't be any)\njinf Array{Int64,1} indices of the visibly infeasible constraints\nnnzj Int number of nonzeros in the sparse Jacobian\nnnzh Int number of nonzeros in the sparse Hessian\nminimize Bool true if optimize == minimize\nislp Bool true if the problem is a linear program\nname String problem name","category":"page"},{"location":"#License-1","page":"Home","title":"License","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This content is released under the MPL2.0 License.","category":"page"},{"location":"#Contents-1","page":"Home","title":"Contents","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"tools/#tools-section-1","page":"Tools","title":"Tools","text":"","category":"section"},{"location":"tools/#Functions-evaluations-1","page":"Tools","title":"Functions evaluations","text":"","category":"section"},{"location":"tools/#","page":"Tools","title":"Tools","text":"After calling one the API functions to get a function value, the number of times that function was called is stored inside the NLPModel. For instance","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"using NLPModels, LinearAlgebra\nnlp = ADNLPModel(x -> dot(x, x), zeros(2))\nfor i = 1:100\n    obj(nlp, rand(2))\nend\nneval_obj(nlp)","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"Some counters are available for all models, some are specific. In particular, there are additional specific counters for the nonlinear least squares models.","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"Counter Description\nneval_obj Objective\nneval_grad Gradient\nneval_cons Constraints\nneval_jcon One constraint - unused\nneval_jgrad Gradient of one constraints - unused\nneval_jac Jacobian\nneval_jprod Product of Jacobian and vector\nneval_jtprod Product of transposed Jacobian and vector\nneval_hess Hessian\nneval_hprod Product of Hessian and vector\nneval_jhprod Product of Hessian of j-th function and vector\nneval_residual Residual function of nonlinear least squares model\nneval_jac_residual Jacobian of the residual\nneval_jprod_residual Product of Jacobian of residual and vector\nneval_jtprod_residual Product of transposed Jacobian of residual and vector\nneval_hess_residual Sum of Hessians of residuals\nneval_jhess_residual Hessian of a residual component\nneval_hprod_residual Product of Hessian of a residual component and vector","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"To get the sum of all counters called for a problem, use sum_counters.","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"using NLPModels, LinearAlgebra\nnlp = ADNLPModel(x -> dot(x, x), zeros(2))\nobj(nlp, rand(2))\ngrad(nlp, rand(2))\nsum_counters(nlp)","category":"page"},{"location":"tools/#Querying-problem-type-1","page":"Tools","title":"Querying problem type","text":"","category":"section"},{"location":"tools/#","page":"Tools","title":"Tools","text":"There are some variable for querying the problem type:","category":"page"},{"location":"tools/#","page":"Tools","title":"Tools","text":"bound_constrained: True for problems with bounded variables and no other constraints.\nequality_constrained: True when problem is constrained only by equalities.\nhas_bounds: True when not all variables are free.\ninequality_constrained: True when problem is constrained by inequalities.\nlinearly_constrained: True when problem is constrained by equalities or inequalities known to be linear.\nunconstrained: True when problem is not constrained.","category":"page"},{"location":"tools/#Docs-1","page":"Tools","title":"Docs","text":"","category":"section"},{"location":"tools/#","page":"Tools","title":"Tools","text":"neval_obj\nneval_grad\nneval_cons\nneval_jcon\nneval_jgrad\nneval_jac\nneval_jprod\nneval_jtprod\nneval_hess\nneval_hprod\nneval_jhprod\nneval_residual\nneval_jac_residual\nneval_jprod_residual\nneval_jtprod_residual\nneval_hess_residual\nneval_jhess_residual\nneval_hprod_residual\nsum_counters\nbound_constrained\nequality_constrained\nhas_bounds\ninequality_constrained\nlinearly_constrained\nunconstrained","category":"page"},{"location":"tools/#NLPModels.neval_obj","page":"Tools","title":"NLPModels.neval_obj","text":"neval_obj(nlp)\n\nGet the number of obj evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_grad","page":"Tools","title":"NLPModels.neval_grad","text":"neval_grad(nlp)\n\nGet the number of grad evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_cons","page":"Tools","title":"NLPModels.neval_cons","text":"neval_cons(nlp)\n\nGet the number of cons evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jcon","page":"Tools","title":"NLPModels.neval_jcon","text":"neval_jcon(nlp)\n\nGet the number of jcon evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jgrad","page":"Tools","title":"NLPModels.neval_jgrad","text":"neval_jgrad(nlp)\n\nGet the number of jgrad evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jac","page":"Tools","title":"NLPModels.neval_jac","text":"neval_jac(nlp)\n\nGet the number of jac evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jprod","page":"Tools","title":"NLPModels.neval_jprod","text":"neval_jprod(nlp)\n\nGet the number of jprod evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jtprod","page":"Tools","title":"NLPModels.neval_jtprod","text":"neval_jtprod(nlp)\n\nGet the number of jtprod evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_hess","page":"Tools","title":"NLPModels.neval_hess","text":"neval_hess(nlp)\n\nGet the number of hess evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_hprod","page":"Tools","title":"NLPModels.neval_hprod","text":"neval_hprod(nlp)\n\nGet the number of hprod evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jhprod","page":"Tools","title":"NLPModels.neval_jhprod","text":"neval_jhprod(nlp)\n\nGet the number of jhprod evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_residual","page":"Tools","title":"NLPModels.neval_residual","text":"neval_residual(nlp)\n\nGet the number of residual evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jac_residual","page":"Tools","title":"NLPModels.neval_jac_residual","text":"nevaljacresidual(nlp)\n\nGet the number of jac evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jprod_residual","page":"Tools","title":"NLPModels.neval_jprod_residual","text":"nevaljprodresidual(nlp)\n\nGet the number of jprod evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jtprod_residual","page":"Tools","title":"NLPModels.neval_jtprod_residual","text":"nevaljtprodresidual(nlp)\n\nGet the number of jtprod evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_hess_residual","page":"Tools","title":"NLPModels.neval_hess_residual","text":"nevalhessresidual(nlp)\n\nGet the number of hess evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_jhess_residual","page":"Tools","title":"NLPModels.neval_jhess_residual","text":"nevaljhessresidual(nlp)\n\nGet the number of jhess evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.neval_hprod_residual","page":"Tools","title":"NLPModels.neval_hprod_residual","text":"nevalhprodresidual(nlp)\n\nGet the number of hprod evaluations.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.sum_counters","page":"Tools","title":"NLPModels.sum_counters","text":"sum_counters(counters)\n\nSum all counters of counters.\n\n\n\n\n\nsum_counters(nlp)\n\nSum all counters of problem nlp.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.bound_constrained","page":"Tools","title":"NLPModels.bound_constrained","text":"bound_constrained(nlp)\nbound_constrained(meta)\n\nReturns whether the problem has bounds on the variables and no other constraints.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.equality_constrained","page":"Tools","title":"NLPModels.equality_constrained","text":"equality_constrained(nlp)\nequality_constrained(meta)\n\nReturns whether the problem's constraints are all equalities. Unconstrained problems return false.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.has_bounds","page":"Tools","title":"NLPModels.has_bounds","text":"has_bounds(nlp)\nhas_bounds(meta)\n\nReturns whether the problem has bounds on the variables.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.inequality_constrained","page":"Tools","title":"NLPModels.inequality_constrained","text":"inequality_constrained(nlp)\ninequality_constrained(meta)\n\nReturns whether the problem's constraints are all inequalities. Unconstrained problems return true.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.linearly_constrained","page":"Tools","title":"NLPModels.linearly_constrained","text":"linearly_constrained(nlp)\nlinearly_constrained(meta)\n\nReturns whether the problem's constraints are known to be all linear.\n\n\n\n\n\n","category":"function"},{"location":"tools/#NLPModels.unconstrained","page":"Tools","title":"NLPModels.unconstrained","text":"unconstrained(nlp)\nunconstrained(meta)\n\nReturns whether the problem in unconstrained.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"NLPModels.jl was created for two purposes:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Allow users to access problem databases in an unified way.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Mainly, this means  CUTEst.jl,  but it also gives access to AMPL  problems,  as well as JuMP defined problems (e.g. as in  OptimizationProblems.jl).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Allow users to create their own problems in the same way.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"As a consequence, optimization methods designed according to the NLPModels API  will accept NLPModels of any provenance.  See, for instance,  JSOSolvers.jl and  NLPModelsIpopt.jl.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The main interface for user defined problems is ADNLPModel, which defines a model easily, using automatic differentiation.","category":"page"},{"location":"tutorial/#ADNLPModel-Tutorial-1","page":"Tutorial","title":"ADNLPModel Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"ADNLPModel is simple to use and is useful for classrooms. It only needs the objective function f and a starting point x^0 to be well-defined. For constrained problems, you'll also need the constraints function c, and the constraints vectors c_L and c_U, such that c_L leq c(x) leq c_U. Equality constraints will be automatically identified as those indices i for which c_L_i = c_U_i.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let's define the famous Rosenbrock function","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"with starting point x^0 = (-1210).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using NLPModels\n\nnlp = ADNLPModel(x->(x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2 , [-1.2; 1.0])","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This is enough to define the model. Let's get the objective function value at x^0, using only nlp.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fx = obj(nlp, nlp.meta.x0)\nprintln(\"fx = $fx\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Done. Let's try the gradient and Hessian.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"gx = grad(nlp, nlp.meta.x0)\nHx = hess(nlp, nlp.meta.x0)\nprintln(\"gx = $gx\")\nprintln(\"Hx = $Hx\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice how only the lower triangle of the Hessian is stored. Also notice that it is dense. This is a current limitation of this model. It doesn't return sparse matrices, so use it with care.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let's do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Given x^0, varepsilon  0, and eta in (01). Set k = 0;\nIf Vert nabla f(x^k) Vert  varepsilon STOP with x^* = x^k;\nCompute d^k = -nabla f(x^k);\nCompute alpha_k in (01 such that f(x^k + alpha_kd^k)  f(x^k) + alpha_keta nabla f(x^k)^Td^k\nDefine x^k+1 = x^k + alpha_kx^k\nUpdate k = k + 1 and go to step 2.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra\n\nfunction steepest(nlp; itmax=100000, eta=1e-4, eps=1e-6, sigma=0.66)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  ∇fx = grad(nlp, x)\n  slope = dot(∇fx, ∇fx)\n  ∇f_norm = sqrt(slope)\n  iter = 0\n  while ∇f_norm > eps && iter < itmax\n    t = 1.0\n    x_trial = x - t * ∇fx\n    f_trial = obj(nlp, x_trial)\n    while f_trial > fx - eta * t * slope\n      t *= sigma\n      x_trial = x - t * ∇fx\n      f_trial = obj(nlp, x_trial)\n    end\n    x = x_trial\n    fx = f_trial\n    ∇fx = grad(nlp, x)\n    slope = dot(∇fx, ∇fx)\n    ∇f_norm = sqrt(slope)\n    iter += 1\n  end\n  optimal = ∇f_norm <= eps\n  return x, fx, ∇f_norm, optimal, iter\nend\n\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln(\"x = $x\")\nprintln(\"fx = $fx\")\nprintln(\"ngx = $ngx\")\nprintln(\"optimal = $optimal\")\nprintln(\"iter = $iter\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Maybe this code is too complicated? If you're in a class you just want to show a Newton step.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"g(x) = grad(nlp, x)\nH(x) = Symmetric(hess(nlp, x), :L)\nx = nlp.meta.x0\nd = -H(x)\\g(x)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"or a few","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"for i = 1:5\n  global x\n  x = x - H(x)\\g(x)\n  println(\"x = $x\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Also, notice how we can reuse the method.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"f(x) = (x[1]^2 + x[2]^2 - 5)^2 + (x[1]*x[2] - 2)^2\nx0 = [3.0; 2.0]\nnlp = ADNLPModel(f, x0)\n\nx, fx, ngx, optimal, iter = steepest(nlp)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"External models can be tested with steepest as well, as long as they implement obj and grad.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"For constrained minimization, you need the constraints vector and bounds too. Bounds on the variables can be passed through a new vector.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using NLPModels # hide\nf(x) = (x[1] - 1.0)^2 + 100*(x[2] - x[1]^2)^2\nx0 = [-1.2; 1.0]\nlvar = [-Inf; 0.1]\nuvar = [0.5; 0.5]\nc(x) = [x[1] + x[2] - 2; x[1]^2 + x[2]^2]\nlcon = [0.0; -Inf]\nucon = [Inf; 1.0]\nnlp = ADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\n\nprintln(\"cx = $(cons(nlp, nlp.meta.x0))\")\nprintln(\"Jx = $(jac(nlp, nlp.meta.x0))\")","category":"page"},{"location":"tutorial/#Manual-model-1","page":"Tutorial","title":"Manual model","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Sometimes you want or need to input your derivatives by hand the easier way to do so is to define a new model. Which functions you want to define depend on which solver you are using. In out test folder, we have the files hs5.jl, hs6.jl, hs10.jl, hs11.jl, hs14.jl and brownden.jl as examples. We present the relevant part of hs6.jl here as well:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"import NLPModels: increment!\nusing NLPModels\n\nmutable struct HS6 <: AbstractNLPModel\n  meta :: NLPModelMeta\n  counters :: Counters\nend\n\nfunction HS6()\n  meta = NLPModelMeta(2, ncon=1, nnzh=1, nnzj=2, x0=[-1.2; 1.0], lcon=[0.0], ucon=[0.0], name=\"hs6\")\n\n  return HS6(meta, Counters())\nend\n\nfunction NLPModels.obj(nlp :: HS6, x :: AbstractVector)\n  increment!(nlp, :neval_obj)\n  return (1 - x[1])^2\nend\n\nfunction NLPModels.grad!(nlp :: HS6, x :: AbstractVector, gx :: AbstractVector)\n  increment!(nlp, :neval_grad)\n  gx .= [2 * (x[1] - 1); 0.0]\n  return gx\nend\n\nfunction NLPModels.hess(nlp :: HS6, x :: AbstractVector; obj_weight=1.0, y=Float64[])\n  increment!(nlp, :neval_hess)\n  w = length(y) > 0 ? y[1] : 0.0\n  return [2.0 * obj_weight - 20 * w   0.0; 0.0 0.0]\nend\n\nfunction NLPModels.hess_coord(nlp :: HS6, x :: AbstractVector; obj_weight=1.0, y=Float64[])\n  increment!(nlp, :neval_hess)\n  w = length(y) > 0 ? y[1] : 0.0\n  return ([1], [1], [2.0 * obj_weight - 20 * w])\nend\n\nfunction NLPModels.hprod!(nlp :: HS6, x :: AbstractVector, v :: AbstractVector, Hv :: AbstractVector; obj_weight=1.0, y=Float64[])\n  increment!(nlp, :neval_hprod)\n  w = length(y) > 0 ? y[1] : 0.0\n  Hv .= [(2.0 * obj_weight - 20 * w) * v[1]; 0.0]\n  return Hv\nend\n\nfunction NLPModels.cons!(nlp :: HS6, x :: AbstractVector, cx :: AbstractVector)\n  increment!(nlp, :neval_cons)\n  cx[1] = 10 * (x[2] - x[1]^2)\n  return cx\nend\n\nfunction NLPModels.jac(nlp :: HS6, x :: AbstractVector)\n  increment!(nlp, :neval_jac)\n  return [-20 * x[1]  10.0]\nend\n\nfunction NLPModels.jac_coord(nlp :: HS6, x :: AbstractVector)\n  increment!(nlp, :neval_jac)\n  return ([1, 1], [1, 2], [-20 * x[1], 10.0])\nend\n\nfunction NLPModels.jprod!(nlp :: HS6, x :: AbstractVector, v :: AbstractVector, Jv :: AbstractVector)\n  increment!(nlp, :neval_jprod)\n  Jv .= [-20 * x[1] * v[1] + 10 * v[2]]\n  return Jv\nend\n\nfunction NLPModels.jtprod!(nlp :: HS6, x :: AbstractVector, v :: AbstractVector, Jtv :: AbstractVector)\n  increment!(nlp, :neval_jtprod)\n  Jtv .= [-20 * x[1]; 10] * v[1]\n  return Jtv\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"hs6 = HS6()\nx = hs6.meta.x0\n(obj(hs6, x), grad(hs6, x))","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cons(hs6, x)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice that we did not define grad nor cons, but grad! and cons! were defined. The default grad and cons uses the inplace version, so there's no need to redefine them.","category":"page"},{"location":"tutorial/#Nonlinear-least-squares-models-1","page":"Tutorial","title":"Nonlinear least squares models","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In addition to the general nonlinear model, we can define the residual function for a nonlinear least-squares problem. In other words, the objective function of the problem is of the form f(x) = tfrac12F(x)^2, and we can define the function F and its derivatives.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"A simple way to define an NLS problem is with ADNLSModel, which uses automatic differentiation.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using NLPModels # hide\nF(x) = [x[1] - 1.0; 10 * (x[2] - x[1]^2)]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2) # 2 nonlinear equations","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"residual(nls, x0)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"jac_residual(nls, x0)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We can also define a linear least squares by passing the matrices that define the problem","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"beginaligned\nmin quad  tfrac12Ax - b^2 \n c_L  leq Cx leq c_U \n ell leq  x leq u\nendaligned","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra # hide\nA = rand(10, 3)\nb = rand(10)\nC = rand(2, 3)\nnls = LLSModel(A, b, C=C, lcon=zeros(2), ucon=zeros(2), lvar=-ones(3), uvar=ones(3))","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"@info norm(jac_residual(nls, zeros(3)) - A)\n@info norm(jac(nls, zeros(3)) - C)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Another way to define a nonlinear least squares is using FeasibilityResidual to consider the constraints of a general nonlinear problem as the residual of the NLS.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"nlp = ADNLPModel(x->0, # objective doesn't matter,\n                 ones(2),\n                 x->[x[1] + x[2] - 1; x[1] * x[2] - 2], # c(x)\n                 zeros(2), zeros(2)) # lcon, ucon\nnls = FeasibilityResidual(nlp)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"s = 0.0\nfor t = 1:100\n  global s\n  x = rand(2)\n  s += norm(residual(nls, x) - cons(nlp, x))\nend\n@info \"s = $s\"","category":"page"}]
}
